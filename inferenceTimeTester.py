import os
import torch
import onnxruntime as ort
import tensorrt as trt
import numpy as np
import time
import pycuda.driver as cuda
import pycuda.autoinit
import subprocess
from tqdm import tqdm  # ì§„í–‰ ìƒíƒœ ê°€ì‹œí™”

# ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
CKPT_PATH = "models/demo_e200.ckpt"
ONNX_PATH = "models/lseg_image_encoder.onnx"
TRT_PATH = "models/lseg_image_encoder.trt"

# Google Drive íŒŒì¼ ID
GDRIVE_FILE_ID = "1FTuHY1xPUkM-5gaDtMfgCl3D0gR89WV7"

# ë”ë¯¸ ì…ë ¥ ë°ì´í„°
dummy_input = torch.randn(1, 3, 480, 480)

### 1. ì²´í¬í¬ì¸íŠ¸ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ ###
def download_ckpt():
    print("[INFO] demo_e200.ckpt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    try:
        subprocess.run(["gdown", f"https://drive.google.com/uc?id={GDRIVE_FILE_ID}", "-O", CKPT_PATH], check=True)
        print("[INFO] ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: demo_e200.ckpt")
    except subprocess.CalledProcessError:
        print("[ERROR] gdownì„ í†µí•´ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ì ‘ ë‹¤ìš´ë¡œë“œ í›„ models í´ë”ì— ë„£ì–´ì£¼ì„¸ìš”.")
        exit(1)

### 2. ONNX ë³€í™˜ ###
def convert_to_onnx():
    print("[INFO] ONNX ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. model_to_onnx.py ì‹¤í–‰í•˜ì—¬ ë³€í™˜í•©ë‹ˆë‹¤...")
    try:
        subprocess.run(["python3", "conversion/model_to_onnx.py"], check=True)
        print("[INFO] ONNX ë³€í™˜ ì™„ë£Œ")
    except subprocess.CalledProcessError:
        print("[ERROR] ONNX ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        exit(1)

### 3. TensorRT ë³€í™˜ ###
def convert_to_trt():
    print("[INFO] TensorRT ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. onnx_to_trt.py ì‹¤í–‰í•˜ì—¬ ë³€í™˜í•©ë‹ˆë‹¤...")
    try:
        subprocess.run(["python3", "conversion/onnx_to_trt.py"], check=True)
        print("[INFO] TensorRT ë³€í™˜ ì™„ë£Œ")
    except subprocess.CalledProcessError:
        print("[ERROR] TensorRT ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        exit(1)

### 4. PyTorch Inference ###
from lseg.lseg_module import LSegModule
from lseg.image_encoder import LSegImageEncoder

def measure_pytorch_inference_time(model, input_tensor, iterations=100):
    print("[INFO] PyTorch ëª¨ë¸ ì¶”ë¡  (GPU) ì‹œì‘...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    input_tensor = input_tensor.to(device)

    with torch.no_grad():
        for _ in tqdm(range(10), desc="Warm-up"):  # Warm-up
            _ = model(input_tensor)

        start_time = time.time()
        for _ in tqdm(range(iterations), desc="PyTorch Inference"):
            _ = model(input_tensor)
        end_time = time.time()

    avg_time = (end_time - start_time) / iterations
    print(f"[RESULT] PyTorch Model Inference Time: {(avg_time*1000):.6f} ms (GPU)")

### 5. ONNX Inference ###
def measure_onnx_inference_time(onnx_path, input_tensor, iterations=100):
    print("[INFO] ONNX ëª¨ë¸ ì¶”ë¡  (GPU) ì‹œì‘...")
    session = ort.InferenceSession(onnx_path, providers=['CUDAExecutionProvider'])
    input_name = session.get_inputs()[0].name
    input_array = input_tensor.numpy()

    for _ in tqdm(range(10), desc="Warm-up"):  # Warm-up
        _ = session.run(None, {input_name: input_array})

    start_time = time.time()
    for _ in tqdm(range(iterations), desc="ONNX Inference"):
        _ = session.run(None, {input_name: input_array})
    end_time = time.time()

    avg_time = (end_time - start_time) / iterations
    print(f"[RESULT] ONNX Model Inference Time: {(avg_time*1000):.6f} ms (GPU)")


### 6. TensorRT Inference ###
def load_tensorrt_engine(trt_engine_path):
    logger = trt.Logger(trt.Logger.WARNING)
    with open(trt_engine_path, "rb") as f, trt.Runtime(logger) as runtime:
        return runtime.deserialize_cuda_engine(f.read())

#output_shape = (1, 512, 240, 240)  # ëª¨ë¸ì˜ ì¶œë ¥ í¬ê¸° í™•ì¸ í•„ìš”

def measure_tensorrt_inference_time(trt_engine_path, input_tensor, iterations=100):
    print("[INFO] TensorRT ëª¨ë¸ ì¶”ë¡  ì‹œì‘...")
    engine = load_tensorrt_engine(trt_engine_path)
    context = engine.create_execution_context()

    input_shape = input_tensor.shape
    output_shape = (1, 512, 240, 240)  # ëª¨ë¸ì˜ ì¶œë ¥ í¬ê¸° í™•ì¸ í•„ìš”
    input_size = int(np.prod(input_shape) * np.dtype(np.float32).itemsize)  # int ë³€í™˜ ì¶”ê°€
    output_size = int(np.prod(output_shape) * np.dtype(np.float32).itemsize)  # int ë³€í™˜ ì¶”ê°€

    d_input = cuda.mem_alloc(input_size)
    d_output = cuda.mem_alloc(output_size)

    stream = cuda.Stream()
    host_input = np.array(input_tensor.numpy(), dtype=np.float32, order='C')
    host_output = np.empty(output_shape, dtype=np.float32, order='C')

    # ì…ë ¥ ë° ì¶œë ¥ í…ì„œ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
    input_name = engine.get_tensor_name(0)
    output_name = engine.get_tensor_name(1)

    print(f"[INFO] TensorRT ì…ë ¥ í…ì„œ ì´ë¦„: {input_name}")
    print(f"[INFO] TensorRT ì¶œë ¥ í…ì„œ ì´ë¦„: {output_name}")

    # ì…ë ¥ ë° ì¶œë ¥ shape ì„¤ì •
    context.set_input_shape(input_name, input_shape)
    print(f"[INFO] ì…ë ¥ shape ì„¤ì • ì™„ë£Œ: {context.get_tensor_shape(input_name)}")

    # ì…ë ¥ê³¼ ì¶œë ¥ì˜ ë©”ëª¨ë¦¬ ì£¼ì†Œ ì„¤ì • (í•„ìˆ˜!)
    context.set_tensor_address(input_name, int(d_input))
    context.set_tensor_address(output_name, int(d_output))

    # Warm-up ë‹¨ê³„
    for _ in tqdm(range(10), desc="Warm-up"):
        cuda.memcpy_htod_async(d_input, host_input, stream)
        context.execute_async_v3(stream.handle)
        cuda.memcpy_dtoh_async(host_output, d_output, stream)
        stream.synchronize()

    # ì„±ëŠ¥ ì¸¡ì •
    start_time = time.time()
    for _ in tqdm(range(iterations), desc="TensorRT Inference"):
        cuda.memcpy_htod_async(d_input, host_input, stream)
        context.execute_async_v3(stream.handle)
        cuda.memcpy_dtoh_async(host_output, d_output, stream)
        stream.synchronize()
    end_time = time.time()

    avg_time = (end_time - start_time) / iterations
    print(f"[RESULT] TensorRT Model Inference Time: {(avg_time*1000):.6f} ms")

### ğŸ”¥ ì‹¤í–‰ ë¶€ë¶„ ###
if __name__ == "__main__":
    print("ğŸš€ [INFO] LSeg Image Encoder Inference Pipeline ì‹œì‘ ğŸš€")

    # 1ï¸âƒ£ ì²´í¬í¬ì¸íŠ¸ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
    if not os.path.exists(CKPT_PATH):
        download_ckpt()

    # 2ï¸âƒ£ ONNX ëª¨ë¸ ë³€í™˜
    if not os.path.exists(ONNX_PATH):
        convert_to_onnx()

    # 3ï¸âƒ£ TensorRT ëª¨ë¸ ë³€í™˜
    if not os.path.exists(TRT_PATH):
        convert_to_trt()

    # 4ï¸âƒ£ PyTorch ëª¨ë¸ ë¡œë“œ
    print("[INFO] PyTorch ëª¨ë¸ ë¡œë“œ ì¤‘...")
    lseg_module = LSegModule.load_from_checkpoint(
        checkpoint_path=CKPT_PATH,
        data_path='../datasets/',
        dataset='ade20k',
        backbone='clip_vitl16_384',
        aux=False,
        num_features=256,
        aux_weight=0,
        se_loss=False,
        se_weight=0,
        base_lr=0,
        batch_size=1,
        max_epochs=0,
        ignore_index=255,
        dropout=0.0,
        scale_inv=False,
        augment=False,
        no_batchnorm=False,
        widehead=True,
        widehead_hr=False,
        map_locatin="cpu",
        arch_option=0,
        block_depth=0,
        activation='lrelu'
    )
    lseg_net = lseg_module.net if hasattr(lseg_module, 'net') else lseg_module
    image_encoder = LSegImageEncoder(lseg_net)

    print("ğŸ“Œ [INFO] ì¶”ë¡  ì‹œê°„ ì¸¡ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    measure_pytorch_inference_time(image_encoder, dummy_input)
    measure_onnx_inference_time(ONNX_PATH, dummy_input)
    measure_tensorrt_inference_time(TRT_PATH, dummy_input)
